{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Frozen TensorFlow model\n",
    "* This notebook is about:\n",
    "    * Freezing a TensorFlow graph\n",
    "    * Loading a frozen model and doing predictions using the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:06.102918Z",
     "start_time": "2019-05-08T02:43:04.579460Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import os\n",
    "import multiprocessing\n",
    "from requests import get\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from mnist import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:06.643492Z",
     "start_time": "2019-05-08T02:43:06.106676Z"
    }
   },
   "outputs": [],
   "source": [
    "obj = mnist()\n",
    "X_train, y_train, X_test, y_test = obj.load_data()\n",
    "\n",
    "# split into validation and test set from the test set alone\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:06.653269Z",
     "start_time": "2019-05-08T02:43:06.645926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Printing shapes of data ===\n",
      "\n",
      "Train data:       (60000, 32, 32, 1) (60000,)\n",
      "Test data:        (7500, 32, 32, 1) (7500,)\n",
      "Validation data:  (2500, 32, 32, 1) (2500,)\n"
     ]
    }
   ],
   "source": [
    "print('==== Printing shapes of data ===')\n",
    "print()\n",
    "print('Train data:      ', X_train.shape, y_train.shape)\n",
    "print('Test data:       ', X_test.shape, y_test.shape)\n",
    "print('Validation data: ', X_validation.shape, y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for freezing a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:06.731527Z",
     "start_time": "2019-05-08T02:43:06.658430Z"
    }
   },
   "outputs": [],
   "source": [
    "class freezeGraph(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "    \n",
    "    # THIS IMPLEMENTATION IS ENTIRELY TAKEN FROM THE METAFLOW BLOG\n",
    "    # MENTIONED IN THE REFERENCE SECTION OF THIS NOTEBOOK\n",
    "    def freeze_graph(self, model_dir, output_node_names):\n",
    "\n",
    "        # The original freeze_graph function\n",
    "\n",
    "        \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "            all its variables into constant \n",
    "\n",
    "        Args:\n",
    "            model_dir: the root folder containing the checkpoint state file\n",
    "            output_node_names: a string, containing all the output node's names, \n",
    "                                comma separated\n",
    "        \"\"\"\n",
    "        # restore graph meta and model/weights\n",
    "\n",
    "        if not tf.gfile.Exists(model_dir):\n",
    "            raise AssertionError(\n",
    "                \"Export directory doesn't exists. Please specify an export \"\n",
    "                \"directory: %s\" % model_dir)\n",
    "\n",
    "        if not output_node_names:\n",
    "            print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "            return -1\n",
    "\n",
    "        # We retrieve our checkpoint fullpath\n",
    "        checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "        input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "\n",
    "        # We precise the file fullname of our freezed graph\n",
    "        absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "        output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "\n",
    "\n",
    "        # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "        clear_devices = True\n",
    "\n",
    "        # We start a session using a temporary fresh Graph\n",
    "#         tf.Session(graph=tf.Graph()) as sess:\n",
    "        # We import the meta graph in the current default Graph\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "        # We restore the weights\n",
    "        saver.restore(self.sess, input_checkpoint)\n",
    "        \n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            self.sess, \n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(',')\n",
    "        )\n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiaize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:06.969492Z",
     "start_time": "2019-05-08T02:43:06.732974Z"
    }
   },
   "outputs": [],
   "source": [
    "f_model = freezeGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:08.190048Z",
     "start_time": "2019-05-08T02:43:06.970827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anuj/.virtualenvs/analytics-3/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./models/mnist/lenet/lenet.ckpt-1\n",
      "WARNING:tensorflow:From <ipython-input-4-7b26c7a6737f>:52: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/anuj/.virtualenvs/analytics-3/lib/python3.5/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "40 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "f_model.freeze_graph('./models/mnist/lenet/', 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to load a frozen model and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:08.205503Z",
     "start_time": "2019-05-08T02:43:08.192118Z"
    }
   },
   "outputs": [],
   "source": [
    "class loadFrozenGraph(object):\n",
    "\n",
    "    def __init__(self, frozen_graph_path):\n",
    "        \n",
    "        # import frozen graph\n",
    "        self.graph = self.import_graph(frozen_graph_path)\n",
    "        # MOST IMPORTANT - pass the loaded graph when creating session\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        # get input and output tensors\n",
    "        self.x = self.graph.get_tensor_by_name('prefix/input:0')\n",
    "        self.y = self.graph.get_tensor_by_name('prefix/output:0')   \n",
    "        \n",
    "    def import_graph(self, frozen_graph_path):\n",
    "            # just a TF way to load a file in desired mode\n",
    "            # we can also use python file api as well, if loading from local FS\n",
    "            # for more, checkout the link in the 'Learnings' section\n",
    "            with tf.gfile.GFile(frozen_graph_path, 'rb') as f:\n",
    "                # initialize a varible with graphdef which is a \n",
    "                # serialized version of the graph\n",
    "                graph_def = tf.GraphDef()\n",
    "                # load graphdef from protobuf file\n",
    "                graph_def.ParseFromString(f.read())\n",
    "            \n",
    "            # create an empty graph - bound in a scope here\n",
    "            # and import the graph def into it\n",
    "            with tf.Graph().as_default() as graph:\n",
    "                tf.import_graph_def(graph_def, name='prefix')\n",
    "\n",
    "            return graph\n",
    "    \n",
    "    def get_tensor_names(self):\n",
    "        # print operations\n",
    "        for op in self.graph.get_operations():\n",
    "            print(op.name)\n",
    "    \n",
    "    def predict_from_frozen_graph(self, X_test):\n",
    "        return np.argmax(self.sess.run(self.y, feed_dict={self.x: X_test}), axis=1)\n",
    "    \n",
    "    def calculate_accuracy(self, y_pred, y_test):\n",
    "        correct_instances = np.where(y_test == y_pred)[0].shape[0]\n",
    "        total_instances = y_test.shape[0]\n",
    "        accuracy = float(correct_instances)/total_instances\n",
    "        print('Accuracy: {}'.format(accuracy*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:08.675988Z",
     "start_time": "2019-05-08T02:43:08.212684Z"
    }
   },
   "outputs": [],
   "source": [
    "model = loadFrozenGraph('./models/mnist/lenet/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using a frozen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T02:43:11.061434Z",
     "start_time": "2019-05-08T02:43:08.695498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.88\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_from_frozen_graph(X_test)\n",
    "model.calculate_accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc\n",
    "* https://cv-tricks.com/how-to/freeze-tensorflow-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb\n",
    "* https://stackoverflow.com/questions/42256938/what-does-tf-gfile-do-in-tensorflow\n",
    "* https://stackoverflow.com/questions/47059848/difference-between-tensorflows-graph-and-graphdef\n",
    "* Things left to explore:\n",
    "    * optimize_frozen_graph api: https://medium.com/@prasadpal107/saving-freezing-optimizing-for-inference-restoring-of-tensorflow-models-b4146deb21b5 , https://stackoverflow.com/questions/45382917/how-to-optimize-for-inference-a-simple-saved-tensorflow-1-0-1-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
